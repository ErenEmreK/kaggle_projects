{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.17.0-dev20240402\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras_nlp\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPU not activated\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replicas: 1\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # detect and init the TPU\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n",
    "    # instantiate a distribution strategy\n",
    "    strategy = tf.distribute.TPUStrategy(tpu)\n",
    "except ValueError:\n",
    "    print(\"TPU not activated\")\n",
    "    strategy = tf.distribute.MirroredStrategy() # Works on CPU, single GPU and multiple GPUs in a single VM.\n",
    "    \n",
    "print(\"replicas:\", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'data'\n",
    "\n",
    "RESULT_DICT = {\n",
    "    0 : \"entailment\",\n",
    "    1 : \"neutral\",\n",
    "    2 : \"contradiction\"\n",
    "}\n",
    "\n",
    "df_train = pd.read_csv(os.path.join(DATA_DIR, \"train.csv\"))\n",
    "df_test = pd.read_csv(os.path.join(DATA_DIR, \"test.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_SPLIT = 0.3\n",
    "TRAIN_SIZE = int(df_train.shape[0]*(1-VALIDATION_SPLIT))\n",
    "BATCH_SIZE = 16 * strategy.num_replicas_in_sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_labels(x, y):\n",
    "    return (x[0], x[1]), y\n",
    "\n",
    "\n",
    "training_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            df_train[['premise','hypothesis']].values,\n",
    "            keras.utils.to_categorical(df_train['label'], num_classes=3)\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "train_dataset = training_dataset.take(TRAIN_SIZE)\n",
    "val_dataset = training_dataset.skip(TRAIN_SIZE)\n",
    "\n",
    "# Apply the preprocessor to every sample of train, val and test data using `map()`.\n",
    "# [`tf.data.AUTOTUNE`](https://www.tensorflow.org/api_docs/python/tf/data/AUTOTUNE) and `prefetch()` are options to tune performance, see\n",
    "# https://www.tensorflow.org/guide/data_performance for details.\n",
    "\n",
    "train_preprocessed = train_dataset.map(split_labels, tf.data.AUTOTUNE).batch(BATCH_SIZE, drop_remainder=True).cache().prefetch(tf.data.AUTOTUNE)\n",
    "val_preprocessed = val_dataset.map(split_labels, tf.data.AUTOTUNE).batch(BATCH_SIZE, drop_remainder=True).cache().prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'keras_nlp' has no attribute 'models'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load a BERT model.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m strategy\u001b[38;5;241m.\u001b[39mscope():\n\u001b[1;32m----> 4\u001b[0m     classifier \u001b[38;5;241m=\u001b[39m \u001b[43mkeras_nlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241m.\u001b[39mBertClassifier\u001b[38;5;241m.\u001b[39mfrom_preset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert_base_multi\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# in distributed training, the recommendation is to scale batch size and learning rate with the numer of workers.\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     classifier\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(\u001b[38;5;241m1e-5\u001b[39m\u001b[38;5;241m*\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mnum_replicas_in_sync),\n\u001b[0;32m      8\u001b[0m                        loss\u001b[38;5;241m=\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mCategoricalCrossentropy(from_logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m      9\u001b[0m                        metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'keras_nlp' has no attribute 'models'"
     ]
    }
   ],
   "source": [
    "# Load a BERT model.\n",
    "\n",
    "with strategy.scope():\n",
    "    classifier = keras_nlp.models.BertClassifier.from_preset(\"bert_base_multi\", num_classes=3)\n",
    "\n",
    "    # in distributed training, the recommendation is to scale batch size and learning rate with the numer of workers.\n",
    "    classifier.compile(optimizer=keras.optimizers.Adam(1e-5*strategy.num_replicas_in_sync),\n",
    "                       loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                       metrics=['accuracy'])\n",
    "    \n",
    "    classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'keras_nlp' has no attribute 'models'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m preset\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistil_bert_base_en_uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Use a shorter sequence length.\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m preprocessor \u001b[38;5;241m=\u001b[39m \u001b[43mkeras_nlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241m.\u001b[39mDistilBertPreprocessor\u001b[38;5;241m.\u001b[39mfrom_preset(preset,\n\u001b[0;32m      6\u001b[0m                                                                    sequence_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m160\u001b[39m,\n\u001b[0;32m      7\u001b[0m                                                                    name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreprocessor_4_tweets\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      8\u001b[0m                                                                   )\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Pretrained classifier.\u001b[39;00m\n\u001b[0;32m     11\u001b[0m classifier \u001b[38;5;241m=\u001b[39m keras_nlp\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mDistilBertClassifier\u001b[38;5;241m.\u001b[39mfrom_preset(preset,\n\u001b[0;32m     12\u001b[0m                                                                preprocessor \u001b[38;5;241m=\u001b[39m preprocessor, \n\u001b[0;32m     13\u001b[0m                                                                num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'keras_nlp' has no attribute 'models'"
     ]
    }
   ],
   "source": [
    "# Load a DistilBERT model.\n",
    "preset= \"distil_bert_base_en_uncased\"\n",
    "\n",
    "# Use a shorter sequence length.\n",
    "preprocessor = keras_nlp.models.DistilBertPreprocessor.from_preset(preset,\n",
    "                                                                   sequence_length=160,\n",
    "                                                                   name=\"preprocessor_4_tweets\"\n",
    "                                                                  )\n",
    "\n",
    "# Pretrained classifier.\n",
    "classifier = keras_nlp.models.DistilBertClassifier.from_preset(preset,\n",
    "                                                               preprocessor = preprocessor, \n",
    "                                                               num_classes=2)\n",
    "\n",
    "classifier.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
